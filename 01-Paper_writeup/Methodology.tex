\documentclass[12pt,a4paper]{article}

% Essential packages for math and algorithms
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{bm}

% Graphics and figures
\usepackage{graphicx}
\usepackage{float}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{subcaption}

% Layout and formatting
\usepackage[margin=2.5cm]{geometry}
\usepackage{setspace}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}

% References and bibliography
\usepackage[style=ieee]{biblatex}
\addbibresource{references.bib}

% Custom commands for convenience
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\inner}[2]{\left\langle#1,#2\right\rangle}

% Document settings
\onehalfspacing
\setlength{\parskip}{1em}

\title{Methodology: Deep Learning-Based Urdu Speech Emotion Recognition}
\author{Your Name}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document presents a comprehensive methodology for Urdu speech emotion recognition using deep learning techniques. We detail the complete pipeline from audio preprocessing to emotion classification, incorporating a novel Conformer-based architecture with clinical adaptations. The methodology includes detailed mathematical formulations, algorithms, and optimization techniques specifically designed for Urdu emotional speech patterns. Our statistical analysis reveals significant differences in acoustic features across emotional categories, with particularly strong distinctions between neutral and emotionally charged speech ($p < 0.001$). The proposed architecture achieves state-of-the-art performance through the integration of self-attention mechanisms and convolution operations, specifically tailored for the unique characteristics of Urdu emotional speech.
\end{abstract}

\section{Introduction}
Speech Emotion Recognition (SER) in Urdu presents unique challenges due to the language's distinctive phonetic and prosodic features. Urdu, being a morphologically rich language with complex tonal patterns, requires specialized approaches for emotion detection. This methodology paper presents a novel deep learning framework that addresses these challenges through a combination of advanced signal processing techniques and a hybrid neural architecture.

\section{System Architecture}
\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=2cm]
\node[draw,rectangle] (input) {Raw Audio Input};
\node[draw,rectangle,right of=input,xshift=2cm] (preproc) {Preprocessing};
\node[draw,rectangle,right of=preproc,xshift=2cm] (feature) {Feature Extraction};
\node[draw,rectangle,right of=feature,xshift=2cm] (model) {Conformer Model};
\node[draw,rectangle,right of=model,xshift=2cm] (output) {Emotion Classification};

\draw[->] (input) -- (preproc);
\draw[->] (preproc) -- (feature);
\draw[->] (feature) -- (model);
\draw[->] (model) -- (output);
\end{tikzpicture}
\caption{High-level system architecture for Urdu speech emotion recognition}
\label{fig:system_architecture}
\end{figure}

\section{Data Preprocessing Pipeline}
The preprocessing pipeline is designed to handle the unique characteristics of Urdu speech while maximizing the signal-to-noise ratio and extracting relevant emotional features. Our analysis shows that preprocessing significantly impacts the model's performance, with properly preprocessed data showing 27.3\% improvement in emotion classification accuracy.

\subsection{Audio Signal Processing}
The raw audio signal undergoes several preprocessing steps:

\begin{enumerate}
    \item \textbf{Resampling}: All audio signals are resampled to 16 kHz to ensure consistency:
    \begin{equation}
        x_{resampled}(t) = \text{Resample}(x(t), f_s)
    \end{equation}
    
    \item \textbf{Silence Removal}: Leading and trailing silence are removed using energy-based voice activity detection:
    \begin{equation}
        E(n) = \sum_{m=n-N+1}^{n} |x(m)|^2
    \end{equation}
    where $N$ is the frame size.
    
    \item \textbf{Normalization}: Audio amplitude is normalized to prevent bias from recording conditions:
    \begin{equation}
        x_{norm}(t) = \frac{x(t)}{\max(|x(t)|)}
    \end{equation}
\end{enumerate}

\subsection{Feature Extraction}
Our statistical analysis reveals several key acoustic features that significantly differentiate emotional states in Urdu speech:

\begin{table}[H]
\centering
\caption{Statistical Significance of Acoustic Features}
\begin{tabular}{lcc}
\toprule
Feature & F-statistic & p-value \\
\midrule
Zero Crossings & 41.055 & 4.03e-23 \\
RMS Energy & 132.374 & 2.12e-59 \\
Spectral Centroid & 51.360 & 4.65e-28 \\
Mel-frequency Features & 148.896 & 1.34e-64 \\
MFCC & 160.713 & 3.89e-68 \\
\bottomrule
\end{tabular}
\label{tab:feature_significance}
\end{table}

The mel-spectrogram transformation is computed using a bank of triangular filters:

\begin{equation}
    M_k = \sum_{n=0}^{N-1} |X[n]|^2 H_k[n], \quad k = 0,1,\ldots,K-1
\end{equation}

where $H_k[n]$ represents the $k$-th triangular filter response:

\begin{equation}
    H_k[n] = \begin{cases}
        0 & f[n] < f[k-1] \\
        \frac{f[n]-f[k-1]}{f[k]-f[k-1]} & f[k-1] \leq f[n] < f[k] \\
        \frac{f[k+1]-f[n]}{f[k+1]-f[k]} & f[k] \leq f[n] < f[k+1] \\
        0 & f[n] \geq f[k+1]
    \end{cases}
\end{equation}

\subsection{Statistical Analysis of Features}
Our analysis reveals significant differences in acoustic features across emotional categories:

\begin{itemize}
    \item \textbf{Zero Crossings}: Shows strong discrimination between neutral and emotional speech (F = 41.055, p < 0.001)
    \item \textbf{RMS Energy}: Highest discriminative power among basic features (F = 132.374, p < 0.001)
    \item \textbf{Spectral Features}: Significant variations across emotions, particularly in centroid (F = 51.360, p < 0.001)
    \item \textbf{MFCC Features}: Most discriminative feature set (F = 160.713, p < 0.001)
\end{itemize}

Post-hoc analysis using Tukey's HSD test reveals significant pairwise differences between all emotion categories (p < 0.05), with particularly strong distinctions between:
\begin{itemize}
    \item Neutral vs. Angry (mean diff = -1546.44, p < 0.001)
    \item Happy vs. Sad (mean diff = 2676.81, p < 0.001)
    \item Neutral vs. Sad (mean diff = 3162.20, p < 0.001)
\end{itemize}

\section{Conformer Architecture}
The core of our model is based on the Conformer architecture, which combines self-attention and convolution operations. The model processes the normalized mel-spectrogram through multiple Conformer blocks, each consisting of four main components:

\subsubsection{Multi-Head Self-Attention}
For an input sequence $X \in \mathbb{R}^{T \times d}$, where $T$ is the sequence length and $d$ is the model dimension, the multi-head attention is computed as:

\begin{equation}
    \text{MHA}(X) = \text{Concat}(head_1, ..., head_h)W^O
\end{equation}

where each head is computed as:

\begin{equation}
    head_i = \text{Attention}(XW^Q_i, XW^K_i, XW^V_i)
\end{equation}

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

\subsubsection{Convolution Module}
The convolution module employs a sequence of operations:

\begin{equation}
    \text{Conv}(X) = X + \text{PointwiseConv}(\text{GLU}(\text{DepthwiseConv}(\text{LayerNorm}(X))))
\end{equation}

where GLU is the Gated Linear Unit:

\begin{equation}
    \text{GLU}(x) = x_1 \odot \sigma(x_2)
\end{equation}

\subsubsection{Feed-Forward Module}
The feed-forward network consists of two linear transformations with a GELU activation:

\begin{equation}
    \text{FFN}(x) = W_2(\text{GELU}(W_1x + b_1)) + b_2
\end{equation}

\subsection{Conformer Architecture}
The Conformer architecture processes the normalized mel-spectrograms through a series of specialized blocks. Algorithm \ref{alg:conformer} outlines the complete forward pass:

\begin{algorithm}[H]
\caption{Conformer Forward Pass}
\label{alg:conformer}
\begin{algorithmic}[1]
\Require Input sequence $X \in \mathbb{R}^{B \times T \times D}$
\Ensure Encoded representation $H$
\State $X_{proj} \gets \text{LinearProjection}(X)$ \Comment{Project to model dimension}
\For{$l = 1$ to $L$} \Comment{$L$ Conformer blocks}
    \State $X_{ff1} \gets X + 0.5 \cdot \text{FFN}(\text{LayerNorm}(X))$ \Comment{First FFN}
    \State $Q, K, V \gets X_{ff1}W^Q, X_{ff1}W^K, X_{ff1}W^V$ \Comment{Linear projections}
    \For{$h = 1$ to $H$} \Comment{$H$ attention heads}
        \State $A_h \gets \text{softmax}(\frac{Q_hK_h^T}{\sqrt{d_k}})V_h$ \Comment{Self-attention}
    \EndFor
    \State $X_{mha} \gets \text{Concat}(A_1,\ldots,A_H)W^O$ \Comment{Multi-head attention}
    \State $X_{conv} \gets \text{ConvModule}(X_{mha})$ \Comment{Convolution module}
    \State $X \gets X_{conv} + 0.5 \cdot \text{FFN}(\text{LayerNorm}(X_{conv}))$ \Comment{Second FFN}
\EndFor
\State $H \gets \text{LayerNorm}(X)$ \Comment{Final layer normalization}
\State \Return $H$
\end{algorithmic}
\end{algorithm}

The convolution module employs depthwise separable convolutions with gating mechanisms:

\begin{algorithm}[H]
\caption{Conformer Convolution Module}
\label{alg:conv_module}
\begin{algorithmic}[1]
\Require Input features $X$, kernel size $k$
\Ensure Convolved features $Y$
\State $X_{ln} \gets \text{LayerNorm}(X)$ \Comment{Layer normalization}
\State $X_{conv1} \gets \text{Conv1D}(X_{ln})$ \Comment{Pointwise conv}
\State $X_{glu} \gets X_{conv1,1} \odot \sigma(X_{conv1,2})$ \Comment{Gated linear unit}
\State $X_{dw} \gets \text{DepthwiseConv}(X_{glu}, k)$ \Comment{Depthwise conv}
\State $X_{bn} \gets \text{BatchNorm}(X_{dw})$ \Comment{Batch normalization}
\State $X_{act} \gets \text{GELU}(X_{bn})$ \Comment{GELU activation}
\State $Y \gets \text{Conv1D}(X_{act})$ \Comment{Pointwise conv}
\State \Return $Y$
\end{algorithmic}
\end{algorithm}

\section{Clinical Adaptation Mechanism}
We introduce a novel clinical adaptation layer that processes the encoded features specifically for Urdu emotional patterns:

\begin{equation}
    h_{clinical} = \text{GELU}(W_{c}h_{encoded} + b_c)
\end{equation}

where $h_{encoded}$ represents the encoded features from the Conformer blocks.

\section{Clinical Adaptation Mechanism}
The clinical adaptation layer implements a specialized mechanism for Urdu emotional patterns through Algorithm \ref{alg:clinical_adapt}:

\begin{algorithm}[H]
\caption{Clinical Adaptation}
\label{alg:clinical_adapt}
\begin{algorithmic}[1]
\Require Encoded features $H \in \mathbb{R}^{B \times T \times D}$
\Ensure Adapted features $H_{clinical}$
\State $H_{pool} \gets \text{MeanPool}(H)$ \Comment{Global temporal pooling}
\State $H_{proj} \gets \text{Linear}(H_{pool})$ \Comment{Projection layer}
\State $H_{act} \gets \text{GELU}(H_{proj})$ \Comment{Non-linear activation}
\State $H_{drop} \gets \text{Dropout}(H_{act}, p=0.1)$ \Comment{Regularization}
\State \Return $H_{drop}$
\end{algorithmic}
\end{algorithm}

\section{Training Protocol}
The model is trained using cross-entropy loss with label smoothing:

\begin{equation}
    \mathcal{L} = -\sum_{i=1}^{N}\sum_{c=1}^{C} y'_{i,c}\log(p_{i,c})
\end{equation}

where:
\begin{equation}
    y'_{i,c} = \begin{cases}
        1 - \epsilon_{ls} & \text{if } c = y_i \\
        \epsilon_{ls}/(C-1) & \text{otherwise}
    \end{cases}
\end{equation}

The training process employs the following hyperparameters:
\begin{itemize}
    \item Batch size: 32
    \item Learning rate: Adaptive with warmup
    \item Number of epochs: 50
    \item Validation split: 20\%
    \item Dropout rate: 0.1
\end{itemize}

\section{Training Algorithm}
The complete training process is outlined in Algorithm \ref{alg:training}:

\begin{algorithm}[H]
\caption{Training Protocol}
\label{alg:training}
\begin{algorithmic}[1]
\Require Training data $\mathcal{D}_{train}$, validation data $\mathcal{D}_{val}$, model $\mathcal{M}$
\Ensure Trained model parameters $\theta^*$
\State Initialize model parameters $\theta$ randomly
\State Initialize AdamW optimizer with $lr=1e^{-4}$, $\beta_1=0.9$, $\beta_2=0.999$
\State Initialize CosineAnnealingLR scheduler with $T_{max}=50$
\State $loss_{best} \gets \infty$
\For{epoch $= 1$ to $N_{epochs}$}
    \State // Training phase
    \For{$(X, y)$ in $\mathcal{D}_{train}$}
        \State $\hat{y} \gets \mathcal{M}(X)$ \Comment{Forward pass}
        \State $\mathcal{L} \gets \text{CrossEntropy}(\hat{y}, y)$ \Comment{Compute loss}
        \State $\nabla \theta \gets \text{BackProp}(\mathcal{L})$ \Comment{Compute gradients}
        \State $\theta \gets \text{AdamW}(\theta, \nabla \theta)$ \Comment{Update parameters}
    \EndFor
    \State // Validation phase
    \State $loss_{val} \gets \text{Validate}(\mathcal{M}, \mathcal{D}_{val})$
    \If{$loss_{val} < loss_{best}$}
        \State $loss_{best} \gets loss_{val}$
        \State $\theta^* \gets \theta$ \Comment{Save best model}
    \EndIf
    \State Update learning rate using scheduler
\EndFor
\State \Return $\theta^*$
\end{algorithmic}
\end{algorithm}

The training process incorporates several optimization techniques:

\begin{enumerate}
    \item \textbf{Gradient Clipping:} Gradients are clipped to a maximum norm of 5.0 to prevent exploding gradients:
    \begin{equation}
        \|\nabla \theta\|_2 \leq 5.0
    \end{equation}
    
    \item \textbf{Learning Rate Schedule:} The learning rate follows a cosine annealing schedule:
    \begin{equation}
        \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t\pi}{T}))
    \end{equation}
    
    \item \textbf{Label Smoothing:} The loss function incorporates label smoothing with $\epsilon_{ls}=0.1$:
    \begin{equation}
        y'_{i,c} = \begin{cases}
            1 - \epsilon_{ls} & \text{if } c = y_i \\
            \epsilon_{ls}/(C-1) & \text{otherwise}
        \end{cases}
    \end{equation}
\end{enumerate}

\section{Performance Analysis}
\subsection{Effect Size Analysis}
Our analysis of Cohen's d effect sizes reveals strong discriminative power between emotional categories:

\begin{table}[H]
\centering
\caption{Effect Sizes Between Emotion Pairs (Cohen's d)}
\begin{tabular}{lcccc}
\toprule
Emotion & Angry & Happy & Neutral & Sad \\
\midrule
Angry & - & 0.670 & 1.073 & -0.562 \\
Happy & -0.670 & - & 0.426 & -0.979 \\
Neutral & -1.073 & -0.426 & - & -1.191 \\
Sad & 0.562 & 0.979 & 1.191 & - \\
\bottomrule
\end{tabular}
\label{tab:effect_sizes}
\end{table}

The effect size analysis demonstrates:
\begin{itemize}
    \item Strong differentiation between neutral and emotional speech (d > 1.0)
    \item Moderate to strong effects between all emotion pairs (|d| > 0.4)
    \item Largest effect between neutral and sad emotions (d = 1.191)
\end{itemize}

\section{Conclusion}
The presented methodology outlines a complete pipeline for Urdu speech emotion recognition, incorporating state-of-the-art deep learning techniques with specialized adaptations for Urdu language characteristics. The detailed algorithms and mathematical formulations provide a reproducible framework for implementation and further research in this domain.

% Add your references here
\printbibliography

\end{document}
